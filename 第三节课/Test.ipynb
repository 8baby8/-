{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先导入所需第三方库\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件路径函数\n",
    "def get_files(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    file_list = []\n",
    "    for filepath, dirnames, filenames in os.walk(dir_path):\n",
    "        # os.walk 函数将递归遍历指定文件夹\n",
    "        for filename in filenames:\n",
    "            # 通过后缀名判断文件类型是否满足要求\n",
    "            if filename.endswith(\".md\"):\n",
    "                # 如果满足要求，将其绝对路径加入到结果列表\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                file_list.append(os.path.join(filepath, filename))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文件函数\n",
    "def get_text(dir_path):\n",
    "    # args：dir_path，目标文件夹路径\n",
    "    # 首先调用上文定义的函数得到目标文件路径列表\n",
    "    file_lst = get_files(dir_path)\n",
    "    # docs 存放加载之后的纯文本对象\n",
    "    docs = []\n",
    "    # 遍历所有目标文件\n",
    "    for one_file in tqdm(file_lst):\n",
    "        file_type = one_file.split('.')[-1]\n",
    "        if file_type == 'md':\n",
    "            loader = UnstructuredMarkdownLoader(one_file)\n",
    "        elif file_type == 'txt':\n",
    "            loader = UnstructuredFileLoader(one_file)\n",
    "        else:\n",
    "            # 如果是不符合条件的文件，直接跳过\n",
    "            continue\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标文件夹\n",
    "tar_dir = [\n",
    "    \"/root/data/InternLM\",\n",
    "    \"/root/data/InternLM-XComposer\",\n",
    "    \"/root/data/lagent\",\n",
    "    \"/root/data/lmdeploy\",\n",
    "    \"/root/data/opencompass\",\n",
    "    \"/root/data/xtuner\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/unstructured/documents/html.py:498: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  rows = body.findall(\"tr\") if body else []\n",
      " 36%|███▌      | 9/25 [00:31<00:30,  1.88s/it]/root/.conda/envs/internlm-demo/lib/python3.10/site-packages/unstructured/documents/html.py:498: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  rows = body.findall(\"tr\") if body else []\n",
      "100%|██████████| 25/25 [00:31<00:00,  1.28s/it]\n",
      "100%|██████████| 9/9 [00:00<00:00, 21.28it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 36.27it/s]\n",
      "100%|██████████| 72/72 [00:02<00:00, 25.04it/s]\n",
      "100%|██████████| 113/113 [00:05<00:00, 19.36it/s]\n",
      "100%|██████████| 26/26 [00:01<00:00, 17.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 加载目标文件\n",
    "docs = []\n",
    "for dir_path in tar_dir:\n",
    "    docs.extend(get_text(dir_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文本进行分块\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=150)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载开源词向量模型\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/data/model/sentence-transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建向量数据库\n",
    "# 定义持久化路径\n",
    "persist_directory = 'data_base/vector_db/chroma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据库\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1\n",
      "a2\n",
      "a3\n",
      "============================运行成功======================\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# 定义 Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"/root/data/model/sentence-transformer\")\n",
    "\n",
    "# 向量数据库持久化路径\n",
    "persist_directory = 'data_base/vector_db/chroma'\n",
    "\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory, \n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b527a6c0e34964b1bd7dc83c58b110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成本地模型的加载\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'作为InternLM，我可以回答你的问题，提供定义和解释，将文本从一种语言翻译成另一种语言，总结文本，生成文本，编写故事，分析情感，提供推荐，开发算法，编写代码以及执行其他任何基于语言的任务。请告诉我你需要我做什么，我会尽力帮助你。'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LLM import InternLM_LLM\n",
    "llm = InternLM_LLM(model_path = \"/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b\")\n",
    "llm.predict(\"你可以做什么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，请问您需要什么样的帮助呢？'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('你好，我想请你帮我个忙')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 我们所构造的 Prompt 模板\n",
    "template = \"\"\"使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。\n",
    "问题: {question}\n",
    "可参考的上下文：\n",
    "···\n",
    "{context}\n",
    "···\n",
    "如果给定的上下文无法让你做出回答，请回答你不知道。\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "# 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=True,chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索问答链回答 question 的结果：\n",
      "根据上下文，InternLM是一种语言模型，由上海人工智能实验室开发。它被设计成一个有帮助、诚实、无害的AI助手。\n"
     ]
    }
   ],
   "source": [
    "# 检索问答链回答效果\n",
    "question = \"什么是InternLM\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(\"检索问答链回答 question 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型回答 question 的结果：\n",
      "书生·浦语\n"
     ]
    }
   ],
   "source": [
    "# 仅 LLM 回答效果\n",
    "result_2 = llm(question)\n",
    "print(\"大模型回答 question 的结果：\")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internlm-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
